{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "71f52749-f532-46c3-975e-d6c9a73d9242",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "# Databricks Notebook: Komplett ETL-pipeline for parkeringsdata (uten Unity Catalog)\n",
    "\n",
    "# ------------------------------\n",
    "# 1. Hent data fra ekstern URL\n",
    "# ------------------------------\n",
    "import requests\n",
    "import json\n",
    "from pyspark.sql import SparkSession\n",
    "from etl_pipeline import sjekk_duplikater, valider_manglende, valider_gyldige_verdier, konverter_timestamp\n",
    "\n",
    "# Hent JSON-data fra en offentlig URL\n",
    "url = \"https://opencom.no/dataset/36ceda99-bbc3-4909-bc52-b05a6d634b3f/resource/d1bdc6eb-9b49-4f24-89c2-ab9f5ce2acce/download/parking.json\"\n",
    "response = requests.get(url)\n",
    "data = response.json()\n",
    "\n",
    "# Konverter JSON-data til Spark DataFrame\n",
    "# (Datatypene blir automatisk inferert, men kan tilpasses manuelt ved behov)\n",
    "df_raw = spark.createDataFrame(data)\n",
    "\n",
    "valider_manglende(df_raw)\n",
    "valider_gyldige_verdier(df_raw)\n",
    "df_deduped = sjekk_duplikater(df_raw)\n",
    "df_cleaned = konverter_timestamp(df_deduped)\n",
    "\n",
    "\n",
    "# --- Datakvalitetssjekker ---\n",
    "from pyspark.sql.functions import col, isnull, count, when\n",
    "import logging\n",
    "\n",
    "# Konfigurer logger\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Sjekk for duplikater basert på primærnøkkelkandidater (f.eks. Sted + Dato + Klokkeslett)\n",
    "# Må gjøres FØR konvertering til timestamp for å bevare originalitet\n",
    "df_raw_with_id = df_raw.withColumn(\n",
    "    \"unique_id\",\n",
    "    concat_ws(\"_\", col(\"Sted\"), col(\"Dato\"), col(\"Klokkeslett\"))\n",
    ")\n",
    "duplicate_count = df_raw_with_id.groupBy(\"unique_id\").agg(count(\"*\").alias(\"count\")).filter(col(\"count\") > 1).count()\n",
    "\n",
    "if duplicate_count > 0:\n",
    "    logger.warning(f\"ADVARSEL: {duplicate_count} duplikate rader funnet i rådataene basert på Sted, Dato, Klokkeslett.\")\n",
    "    # Du kan velge å filtrere bort duplikater her, f.eks.:\n",
    "    df_raw = df_raw_with_id.dropDuplicates([\"Sted\", \"Dato\", \"Klokkeslett\"]).drop(\"unique_id\")\n",
    "    logger.info(\"Duplikater fjernet.\")\n",
    "else:\n",
    "    logger.info(\"Ingen duplikater funnet i rådataene.\")\n",
    "    df_raw = df_raw_with_id.drop(\"unique_id\") # Fjern midlertidig kolonne hvis ingen duplikater\n",
    "\n",
    "# Sjekk for manglende nøkkelverdier (f.eks. Sted, Dato, Klokkeslett, Antall_ledige_plasser)\n",
    "for column_name in [\"Sted\", \"Dato\", \"Klokkeslett\", \"Antall_ledige_plasser\"]:\n",
    "    missing_count = df_raw.filter(isnull(col(column_name))).count()\n",
    "    if missing_count > 0:\n",
    "        logger.error(f\"FEIL: {missing_count} rader har manglende verdier i '{column_name}' kolonnen.\")\n",
    "        # Avhengig av din policy, kan du:\n",
    "        # 1. Droppe rader med manglende nøkkelverdier:\n",
    "        # df_raw = df_raw.na.drop(subset=[column_name])\n",
    "        # logger.info(f\"Rader med manglende verdier i '{column_name}' er fjernet.\")\n",
    "        # 2. Fylle med en standardverdi:\n",
    "        # df_raw = df_raw.fillna({column_name: \"UKJENT\"})\n",
    "        # 3. Kaste en feil og stoppe pipeline (anbefalt for kritiske feil):\n",
    "        raise ValueError(f\"Kritisk feil: Manglende verdier i '{column_name}'. Stopper pipeline.\")\n",
    "    else:\n",
    "        logger.info(f\"Ingen manglende verdier funnet i '{column_name}' kolonnen.\")\n",
    "\n",
    "# Sjekk for korrekte datatyper (Spark infererer, men det kan være lurt å verifisere)\n",
    "# For 'Antall_ledige_plasser', sjekk at den er numerisk og ikke negativ\n",
    "try:\n",
    "    # Kast en feil hvis det er ikke-numeriske verdier som ikke kan castes\n",
    "    # Dette er mer robust hvis du forventer integer/long\n",
    "    df_raw.select(col(\"Antall_ledige_plasser\").cast(\"int\")).collect()\n",
    "    negative_count = df_raw.filter(col(\"Antall_ledige_plasser\") < 0).count()\n",
    "    if negative_count > 0:\n",
    "        logger.warning(f\"ADVARSEL: {negative_count} rader har negative verdier i 'Antall_ledige_plasser'.\")\n",
    "        raise ValueError(f\"Kritisk feil: 'Antall_ledige_plasser' inneholder negative verdier. Stopper pipeline.\")\n",
    "    else:\n",
    "        logger.info(\"Alle 'Antall_ledige_plasser' er gyldige (ikke negative).\")\n",
    "\n",
    "except Exception as e:\n",
    "    logger.error(f\"FEIL: 'Antall_ledige_plasser' kan ikke konverteres til numerisk type. Detaljer: {e}\")\n",
    "    raise TypeError(f\"Kritisk feil: 'Antall_ledige_plasser' er ikke numerisk. Stopper pipeline.\")\n",
    "\n",
    "# Fortsett med rensing og transformasjon kun hvis datakvaliteten er akseptabel\n",
    "# df_cleaned = df_raw.withColumn(...)\n",
    "\n",
    "# ------------------------------------------\n",
    "# 2. Rens og konverter dato/klokkeslett\n",
    "# ------------------------------------------\n",
    "from pyspark.sql.functions import to_timestamp, concat_ws\n",
    "\n",
    "# Lag en timestamp-kolonne fra Dato og Klokkeslett, og fjern de gamle kolonnene\n",
    "df_cleaned = df_raw.withColumn(\n",
    "    \"timestamp\",\n",
    "    to_timestamp(concat_ws(\" \", df_raw[\"Dato\"], df_raw[\"Klokkeslett\"]), \"dd.MM.yyyy HH:mm\")\n",
    ").drop(\"Dato\", \"Klokkeslett\")\n",
    "\n",
    "from pyspark.sql.functions import count\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 3. Lagre staging-tabell (bronse)\n",
    "# ----------------------------------\n",
    "# Append for å bevare historiske data (bronse = ubehandlet rådata)\n",
    "bronze_path = \"/mnt/bronze/staging_parking\"\n",
    "df_cleaned.write.format(\"delta\").mode(\"append\").option(\"mergeSchema\", \"true\").save(bronze_path)\n",
    "spark.sql(f\"CREATE TABLE IF NOT EXISTS default.staging_parking USING DELTA LOCATION '{bronze_path}'\")\n",
    "\n",
    "# ----------------------------------\n",
    "# 4. Dimensjonstabell: Parkering\n",
    "# ----------------------------------\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Hent eksisterende tabell hvis den finnes, ellers None\n",
    "existing_dim_parkering = spark.table(\"default.dim_parkering\") if \"default.dim_parkering\" in [t.name for t in spark.catalog.listTables(\"default\")] else None\n",
    "\n",
    "# Finn nye unike parkeringsplasser og gi kolonnene mer beskrivende navn\n",
    "new_dim_parkering = df_cleaned.select(\"Sted\", \"Latitude\", \"Longitude\").distinct()\n",
    "new_dim_parkering = new_dim_parkering.withColumnRenamed(\"Sted\", \"Parkering_navn\")\n",
    "\n",
    "# Vi antar at det alltid skal være nøyaktig 9 parkeringsplasser. Skriv bare hvis antallet ikke stemmer.\n",
    "if existing_dim_parkering is None or existing_dim_parkering.count() != 9:\n",
    "    new_dim_parkering.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"default.dim_parkering\")\n",
    "\n",
    "# ----------------------------------\n",
    "# 5. Dimensjonstabell: Tid\n",
    "# ----------------------------------\n",
    "from pyspark.sql.functions import to_date, hour, minute\n",
    "\n",
    "# Trekk ut dato, time og minutt fra timestamp\n",
    "dim_tid_batch = df_cleaned.select(\n",
    "    to_date(\"timestamp\").alias(\"dato\"),\n",
    "    hour(\"timestamp\").alias(\"time\"),\n",
    "    minute(\"timestamp\").alias(\"minutt\")\n",
    ").distinct()\n",
    "\n",
    "# Skriv kun nye tidspunkt til tabellen\n",
    "if \"default.dim_tid\" in [t.name for t in spark.catalog.listTables(\"default\")]:\n",
    "    existing_dim_tid = spark.table(\"default.dim_tid\")\n",
    "    delta_tid = dim_tid_batch.subtract(existing_dim_tid)\n",
    "    if delta_tid.count() > 0:\n",
    "        delta_tid.write.format(\"delta\").mode(\"append\").saveAsTable(\"default.dim_tid\")\n",
    "else:\n",
    "    dim_tid_batch.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"default.dim_tid\")\n",
    "\n",
    "# ----------------------------------\n",
    "# 6. Faktatabell: Parkeringskapasitet\n",
    "# ----------------------------------\n",
    "# Hent 9 observasjoner per kjøring (én per lokasjon)\n",
    "new_fakt = df_cleaned.select(\n",
    "    to_date(\"timestamp\").alias(\"dato\"),\n",
    "    \"Sted\",\n",
    "    \"Antall_ledige_plasser\",\n",
    "    \"timestamp\"\n",
    ")\n",
    "\n",
    "# Append for å lagre hver kjørings observasjoner\n",
    "new_fakt.write.format(\"delta\").mode(\"append\").saveAsTable(\"default.fakt_parkering\")\n",
    "\n",
    "# ----------------------------------\n",
    "# 7. Visualisering: Utvikling per lokasjon (én dag)\n",
    "# ----------------------------------\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import matplotlib.dates as mdates\n",
    "\n",
    "# --- Hent full historikk fra faktatabellen ---\n",
    "df_plot = spark.table(\"default.fakt_parkering\")\n",
    "\n",
    "# Konverter til Pandas\n",
    "df_pd = df_plot.toPandas()\n",
    "\n",
    "# --- Konverter timestamp til datetime ---\n",
    "df_pd['datetime'] = pd.to_datetime(df_pd['timestamp'])\n",
    "\n",
    "# --- Velg én dag for visualisering ---\n",
    "valgt_dato = \"2025-06-11\"\n",
    "df_pd['dato'] = df_pd['datetime'].dt.date\n",
    "df_plot_dag = df_pd[df_pd['dato'] == pd.to_datetime(valgt_dato).date()]\n",
    "\n",
    "# Sjekk om vi har noen data\n",
    "unike_steder = df_plot_dag['Sted'].unique()\n",
    "\n",
    "if len(unike_steder) > 0:\n",
    "    fig, axs = plt.subplots(len(unike_steder), 1, figsize=(12, 4 * len(unike_steder)), sharex=True)\n",
    "\n",
    "    if len(unike_steder) == 1:\n",
    "        axs = [axs]\n",
    "\n",
    "    for ax, sted in zip(axs, unike_steder):\n",
    "        subset = df_plot_dag[df_plot_dag['Sted'] == sted].sort_values('datetime')\n",
    "        ax.plot(subset['datetime'], subset['Antall_ledige_plasser'], marker='o', linestyle='-')\n",
    "        ax.set_title(f\"Ledige plasser - {sted}\")\n",
    "        ax.set_ylabel(\"Antall ledige\")\n",
    "        ax.grid(True)\n",
    "\n",
    "\n",
    "    axs[-1].xaxis.set_major_locator(mdates.AutoDateLocator())\n",
    "    axs[-1].xaxis.set_major_formatter(mdates.DateFormatter(\"%H:%M\"))\n",
    "\n",
    "    plt.xlabel(\"Tid (klokkeslett)\")\n",
    "    fig.autofmt_xdate(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Ingen data funnet for valgt dato:\", valgt_dato)\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Stavanger_parkering.py",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
