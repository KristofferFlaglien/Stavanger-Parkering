{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "71f52749-f532-46c3-975e-d6c9a73d9242",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "# Databricks Notebook: Komplett ETL-pipeline for parkeringsdata (uten Unity Catalog)\n",
    "\n",
    "# ------------------------------\n",
    "# 1. Hent data fra ekstern URL\n",
    "# ------------------------------\n",
    "import requests\n",
    "import json\n",
    "from pyspark.sql import SparkSession\n",
    "from tests.etl_pipeline import sjekk_duplikater, valider_manglende, valider_gyldige_verdier, konverter_timestamp\n",
    "from pyspark.sql.types import IntegerType\n",
    "from pyspark.sql.functions import when, col, trim, lower\n",
    "\n",
    "\n",
    "\n",
    "# Hent JSON-data fra en offentlig URL\n",
    "url = \"https://opencom.no/dataset/36ceda99-bbc3-4909-bc52-b05a6d634b3f/resource/d1bdc6eb-9b49-4f24-89c2-ab9f5ce2acce/download/parking.json\"\n",
    "\n",
    "# Hent data med robust feilhåndtering\n",
    "try:\n",
    "    response = requests.get(url)\n",
    "    response.raise_for_status()\n",
    "    data = response.json()\n",
    "except Exception as e:\n",
    "    # Avslutt notebooken med feilmelding hvis noe går galt\n",
    "    dbutils.notebook.exit(f\"Feil ved henting av data: {str(e)}\")\n",
    "\n",
    "# Konverter JSON-data til Spark DataFrame\n",
    "# (Datatypene blir automatisk inferert, men kan tilpasses manuelt ved behov)\n",
    "df_raw = spark.createDataFrame(data)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f997f647-5bdb-4b36-92a2-ac02c5fee674",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 2. Lagre rådata (bronse)\n",
    "# ----------------------------------\n",
    "# Append for å bevare historiske data (bronse = ubehandlet rådata)\n",
    "\n",
    "from delta.tables import DeltaTable\n",
    "from pyspark.sql.functions import current_timestamp\n",
    "\n",
    "\n",
    "bronze_path = \"/mnt/bronze/staging_parking\"\n",
    "\n",
    "\n",
    "df_raw.write.format(\"delta\").mode(\"append\").option(\"mergeSchema\", \"true\").save(bronze_path)\n",
    "spark.sql(f\"CREATE TABLE IF NOT EXISTS default.staging_parking USING DELTA LOCATION '{bronze_path}'\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3ef1b38f-919b-4a65-ae36-571bb7ffa8b5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# --------------------------------------------------------------\n",
    "# 3. Kvalitetssikre data og konverter dato/klokkeslett (silver)\n",
    "# --------------------------------------------------------------\n",
    "\n",
    "from pyspark.sql.functions import when, col, trim, lower\n",
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "\n",
    "# Les inn staging-parking-tabellen (bronse) på nytt, slik at koden er uavhengig av df_raw i minne\n",
    "df_raw = spark.table(\"default.staging_parking\")\n",
    "\n",
    "\n",
    "# Konverter \"Fullt\" til 0 og cast antall_ledige_plasser til Integer\n",
    "df_raw = df_raw.withColumn(\n",
    "    \"Antall_ledige_plasser\",\n",
    "    when(lower(trim(col(\"Antall_ledige_plasser\"))) == \"fullt\", 0)\n",
    "    .otherwise(col(\"Antall_ledige_plasser\"))\n",
    "    .cast(IntegerType())\n",
    ")\n",
    "\n",
    "\n",
    "valider_manglende(df_raw) # sjekk at det ikke finnes manglende verdier i rådataene \n",
    "valider_gyldige_verdier(df_raw) # sjekk at verdiene er gyldige \n",
    "df_deduped = sjekk_duplikater(df_raw) # sjekk for duplikater\n",
    "df_cleaned = konverter_timestamp(df_deduped) # rens og konverter dato og klokkelett\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# df_cleaned er nå klar til å bli lagret i silver\n",
    "\n",
    "silver_path = \"/mnt/silver/parking_cleaned\"\n",
    "\n",
    "\n",
    "(\n",
    "    df_cleaned\n",
    "    .write\n",
    "    .format(\"delta\")\n",
    "    .mode(\"append\")  # append = keep history, overwrite = replace batch\n",
    "    .option(\"mergeSchema\", \"true\")\n",
    "    .save(silver_path)\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# Register as a Hive Metastore table for easy SQL queries\n",
    "spark.sql(f\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS default.parking_cleaned\n",
    "USING DELTA\n",
    "LOCATION '{silver_path}'\n",
    "\"\"\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8e91ecb0-562f-4fb2-a4c9-7563548d6eb0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ----------------------------------\n",
    "# 4. Dimensjonstabell: Parkering\n",
    "# ----------------------------------\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "\n",
    "# Les inn parking_cleaned (silver) på nytt, slik at koden er uavhengig av df_cleaned i minne\n",
    "df_cleaned = spark.table(\"default.parking_cleaned\")\n",
    "\n",
    "# Hent eksisterende tabell hvis den finnes, ellers None\n",
    "existing_dim_parkering = spark.table(\"default.dim_parkering\") if \"default.dim_parkering\" in [t.name for t in spark.catalog.listTables(\"default\")] else None\n",
    "\n",
    "# Finn nye unike parkeringsplasser og gi kolonnene mer beskrivende navn\n",
    "new_dim_parkering = df_cleaned.select(\"Sted\", \"Latitude\", \"Longitude\").distinct()\n",
    "new_dim_parkering = new_dim_parkering.withColumnRenamed(\"Sted\", \"Parkering_navn\")\n",
    "\n",
    "\n",
    "# Bruk MERGE for å oppdatere dimensjonstabellen uten duplikater\n",
    "\n",
    "# gir lik funksjonalitet som å bruke DataFrame-basert delta merge med DeltaTable.forName(...).alias(...).merge(...)\n",
    "# kun forskjell i syntaks\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "MERGE INTO default.dim_parkering AS target\n",
    "USING (SELECT DISTINCT Sted AS Parkering_navn, Latitude, Longitude FROM default.staging_parking) AS source\n",
    "ON target.Parkering_navn = source.Parkering_navn\n",
    "WHEN NOT MATCHED THEN INSERT *\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "465af1e6-d787-4b32-b438-92ffe02495d1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ----------------------------------\n",
    "# 5. Dimensjonstabell: Tid\n",
    "# ----------------------------------\n",
    "\n",
    "\n",
    "\n",
    "from pyspark.sql.functions import to_date, hour, minute\n",
    "from pyspark.sql import Row\n",
    "\n",
    "\n",
    "# Les inn parking_cleaned (silver) på nytt, slik at koden er uavhengig av df_cleaned i minne\n",
    "df_cleaned = spark.table(\"default.parking_cleaned\")\n",
    "\n",
    "# Bygg batch kun basert på nyeste data\n",
    "dim_tid_batch = df_cleaned.select(\n",
    "    to_date(\"timestamp\").alias(\"dato\"),\n",
    "    hour(\"timestamp\").alias(\"time\"),\n",
    "    minute(\"timestamp\").alias(\"minutt\")\n",
    ").distinct()\n",
    "\n",
    "\n",
    "# gjør tabellen tilgjengelig for Spark SQL\n",
    "dim_tid_batch.createOrReplaceTempView(\"new_dim_tid\")\n",
    "\n",
    "\n",
    "# MERGE for å legge til nye tidspunkter uten duplikater\n",
    "\n",
    "# # gir lik funksjonalitet som å bruke DataFrame-basert delta merge med DeltaTable.forName(...).alias(...).merge(...)\n",
    "# kun forskjell i syntaks\n",
    "# alle delta tables blir lest inn i Spark som DataFrames når man gjør spørringer på dem\n",
    "\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "MERGE INTO default.dim_tid AS target\n",
    "USING new_dim_tid AS source\n",
    "ON target.dato = source.dato AND target.time = source.time AND target.minutt = source.minutt\n",
    "WHEN NOT MATCHED THEN INSERT *\n",
    "\"\"\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "10965ebd-3416-4c55-bc14-9d5f59c37177",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ------------------------------------------\n",
    "# 6. Faktatabell: Parkeringskapasitet (gold)\n",
    "\n",
    "# gold-layer\n",
    "\n",
    "# --------------------------------------------\n",
    "from pyspark.sql.functions import to_date, hour, minute, date_format, to_timestamp, when, lit\n",
    "from pyspark.sql.functions import col, max as max_\n",
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "\n",
    "# Hent 9 observasjoner per kjøring (én per lokasjon)\n",
    "\n",
    "# spark.sql(\"TRUNCATE TABLE fakt_parkering\")\n",
    "\n",
    "# Prepare base DataFrame\n",
    "\n",
    "\n",
    "\n",
    "# Les inn parking_cleaned (silver) på nytt, slik at koden er uavhengig av df_cleaned i minne\n",
    "df_cleaned = spark.table(\"default.parking_cleaned\")\n",
    "\n",
    "new_fakt = df_cleaned.select(\n",
    "    to_date(\"timestamp\").alias(\"dato\"),\n",
    "    date_format(to_timestamp(\"timestamp\"), \"HH:mm\").alias(\"klokkeslett\"),\n",
    "    col(\"Sted\"),\n",
    "    col(\"Antall_ledige_plasser\").cast(IntegerType()).alias(\"Antall_ledige_plasser\"),\n",
    "    col(\"timestamp\")\n",
    ")\n",
    "\n",
    "\n",
    "gold_path = \"/mnt/gold/fakt_parkering\"\n",
    "\n",
    "(\n",
    "    new_fakt\n",
    "    .write\n",
    "    .format(\"delta\")\n",
    "    .mode(\"append\")  # append for incremental loads\n",
    "    .option(\"mergeSchema\", \"true\")\n",
    "    .save(gold_path)\n",
    ")\n",
    "\n",
    "\n",
    "# Register as Gold table in Hive Metastore\n",
    "spark.sql(f\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS default.fakt_parkering\n",
    "USING DELTA\n",
    "LOCATION '{gold_path}'\n",
    "\"\"\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# OPTIMIZE forbedrer lagring og spørringsytelse\n",
    "\n",
    "# ZORDER BY (timestamp) organiserer dataene fysisk på disken slik at rader med lignende verdier for 'timestamp' lagres nær hverandre. \n",
    "# dette gjør det mye raskere å hente ut data basert på tid, spesielt ved filtrering på dato/tid\n",
    "\n",
    "# spark.sql(\"OPTIMIZE default.fakt_parkering ZORDER BY (timestamp)\")\n",
    "\n",
    "\n",
    "new_fakt.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"default.fakt_parkering_siste9\")\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 7242190272179097,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Stavanger_parkering",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
