{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "71f52749-f532-46c3-975e-d6c9a73d9242",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "# Databricks Notebook: Komplett ETL-pipeline for parkeringsdata (uten Unity Catalog)\n",
    "\n",
    "# ------------------------------\n",
    "# 1. Hent data fra ekstern URL\n",
    "# ------------------------------\n",
    "import requests\n",
    "import json\n",
    "from pyspark.sql import SparkSession\n",
    "from tests.etl_pipeline import sjekk_duplikater, valider_manglende, valider_gyldige_verdier, konverter_timestamp\n",
    "\n",
    "# Hent JSON-data fra en offentlig URL\n",
    "url = \"https://opencom.no/dataset/36ceda99-bbc3-4909-bc52-b05a6d634b3f/resource/d1bdc6eb-9b49-4f24-89c2-ab9f5ce2acce/download/parking.json\"\n",
    "response = requests.get(url)\n",
    "data = response.json()\n",
    "\n",
    "# Konverter JSON-data til Spark DataFrame\n",
    "# (Datatypene blir automatisk inferert, men kan tilpasses manuelt ved behov)\n",
    "df_raw = spark.createDataFrame(data)\n",
    "\n",
    "\"\"\"\"\n",
    "# --- Datakvalitetssjekker ---\n",
    "from pyspark.sql.functions import col, isnull, count, when, concat_ws, to_timestamp, to_date, hour, minute\n",
    "import logging\n",
    "\n",
    "# Konfigurer logger\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Sjekk for duplikater basert på primærnøkkelkandidater (f.eks. Sted + Dato + Klokkeslett)\n",
    "# Må gjøres FØR konvertering til timestamp for å bevare originalitet\n",
    "df_raw_with_id = df_raw.withColumn(\n",
    "    \"unique_id\",\n",
    "    concat_ws(\"_\", col(\"Sted\"), col(\"Dato\"), col(\"Klokkeslett\"))\n",
    ")\n",
    "duplicate_count = df_raw_with_id.groupBy(\"unique_id\").agg(count(\"*\").alias(\"count\")).filter(col(\"count\") > 1).count()\n",
    "\n",
    "if duplicate_count > 0:\n",
    "    logger.warning(f\"ADVARSEL: {duplicate_count} duplikate rader funnet i rådataene basert på Sted, Dato, Klokkeslett.\")\n",
    "    # Du kan velge å filtrere bort duplikater her, f.eks.:\n",
    "    df_raw = df_raw_with_id.dropDuplicates([\"Sted\", \"Dato\", \"Klokkeslett\"]).drop(\"unique_id\")\n",
    "    logger.info(\"Duplikater fjernet.\")\n",
    "else:\n",
    "    logger.info(\"Ingen duplikater funnet i rådataene.\")\n",
    "    df_raw = df_raw_with_id.drop(\"unique_id\") # Fjern midlertidig kolonne hvis ingen duplikater\n",
    "\n",
    "# Sjekk for manglende nøkkelverdier (f.eks. Sted, Dato, Klokkeslett, Antall_ledige_plasser)\n",
    "for column_name in [\"Sted\", \"Dato\", \"Klokkeslett\", \"Antall_ledige_plasser\"]:\n",
    "    missing_count = df_raw.filter(isnull(col(column_name))).count()\n",
    "    if missing_count > 0:\n",
    "        logger.error(f\"FEIL: {missing_count} rader har manglende verdier i '{column_name}' kolonnen.\")\n",
    "        # Avhengig av din policy, kan du:\n",
    "        # 1. Droppe rader med manglende nøkkelverdier:\n",
    "        # df_raw = df_raw.na.drop(subset=[column_name])\n",
    "        # logger.info(f\"Rader med manglende verdier i '{column_name}' er fjernet.\")\n",
    "        # 2. Fylle med en standardverdi:\n",
    "        # df_raw = df_raw.fillna({column_name: \"UKJENT\"})\n",
    "        # 3. Kaste en feil og stoppe pipeline (anbefalt for kritiske feil):\n",
    "        raise ValueError(f\"Kritisk feil: Manglende verdier i '{column_name}'. Stopper pipeline.\")\n",
    "    else:\n",
    "        logger.info(f\"Ingen manglende verdier funnet i '{column_name}' kolonnen.\")\n",
    "\n",
    "# Sjekk for korrekte datatyper (Spark infererer, men det kan være lurt å verifisere)\n",
    "# For 'Antall_ledige_plasser', sjekk at den er numerisk og ikke negativ\n",
    "try:\n",
    "    # Kast en feil hvis det er ikke-numeriske verdier som ikke kan castes\n",
    "    # Dette er mer robust hvis du forventer integer/long\n",
    "    df_raw.select(col(\"Antall_ledige_plasser\").cast(\"int\")).collect()\n",
    "    negative_count = df_raw.filter(col(\"Antall_ledige_plasser\") < 0).count()\n",
    "    if negative_count > 0:\n",
    "        logger.warning(f\"ADVARSEL: {negative_count} rader har negative verdier i 'Antall_ledige_plasser'.\")\n",
    "        raise ValueError(f\"Kritisk feil: 'Antall_ledige_plasser' inneholder negative verdier. Stopper pipeline.\")\n",
    "    else:\n",
    "        logger.info(\"Alle 'Antall_ledige_plasser' er gyldige (ikke negative).\")\n",
    "\n",
    "except Exception as e:\n",
    "    logger.error(f\"FEIL: 'Antall_ledige_plasser' kan ikke konverteres til numerisk type. Detaljer: {e}\")\n",
    "    raise TypeError(f\"Kritisk feil: 'Antall_ledige_plasser' er ikke numerisk. Stopper pipeline.\")\n",
    "\n",
    "# Fortsett med rensing og transformasjon kun hvis datakvaliteten er akseptabel\n",
    "# df_cleaned = df_raw.withColumn(...)\n",
    "\n",
    "# ------------------------------------------\n",
    "# 2. Rens og konverter dato/klokkeslett\n",
    "# ------------------------------------------\n",
    "from pyspark.sql.functions import to_timestamp, concat_ws\n",
    "\n",
    "# Lag en timestamp-kolonne fra Dato og Klokkeslett, og fjern de gamle kolonnene\n",
    "df_cleaned = df_raw.withColumn(\n",
    "    \"timestamp\",\n",
    "    to_timestamp(concat_ws(\" \", df_raw[\"Dato\"], df_raw[\"Klokkeslett\"]), \"dd.MM.yyyy HH:mm\")\n",
    ").drop(\"Dato\", \"Klokkeslett\")\n",
    "\n",
    "from pyspark.sql.functions import count\n",
    "\"\"\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f997f647-5bdb-4b36-92a2-ac02c5fee674",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ----------------------------------------------------\n",
    "# 2. Kvalitetssikre data og konverter dato/klokkeslett\n",
    "# ----------------------------------------------------\n",
    "\n",
    "\n",
    "valider_manglende(df_raw) # sjekk at det ikke finnes manglende verdier å rådataene \n",
    "valider_gyldige_verdier(df_raw) # sjekk at verdiene er gyldige \n",
    "df_deduped = sjekk_duplikater(df_raw) # sjekk for duplikater\n",
    "df_cleaned = konverter_timestamp(df_deduped) # rens og konverter dato og klokkelett\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3ef1b38f-919b-4a65-ae36-571bb7ffa8b5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 3. Lagre staging-tabell (bronse)\n",
    "# ----------------------------------\n",
    "# Append for å bevare historiske data (bronse = ubehandlet rådata)\n",
    "bronze_path = \"/mnt/bronze/staging_parking\"\n",
    "df_raw.write.format(\"delta\").mode(\"append\").option(\"mergeSchema\", \"true\").save(bronze_path)\n",
    "spark.sql(f\"CREATE TABLE IF NOT EXISTS default.staging_parking USING DELTA LOCATION '{bronze_path}'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8e91ecb0-562f-4fb2-a4c9-7563548d6eb0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ----------------------------------\n",
    "# 4. Dimensjonstabell: Parkering\n",
    "# ----------------------------------\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Hent eksisterende tabell hvis den finnes, ellers None\n",
    "existing_dim_parkering = spark.table(\"default.dim_parkering\") if \"default.dim_parkering\" in [t.name for t in spark.catalog.listTables(\"default\")] else None\n",
    "\n",
    "# Finn nye unike parkeringsplasser og gi kolonnene mer beskrivende navn\n",
    "new_dim_parkering = df_cleaned.select(\"Sted\", \"Latitude\", \"Longitude\").distinct()\n",
    "new_dim_parkering = new_dim_parkering.withColumnRenamed(\"Sted\", \"Parkering_navn\")\n",
    "\n",
    "# Vi antar at det alltid skal være nøyaktig 9 parkeringsplasser. Skriv bare hvis antallet ikke stemmer.\n",
    "if existing_dim_parkering is None or existing_dim_parkering.count() != 9:\n",
    "    new_dim_parkering.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"default.dim_parkering\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "465af1e6-d787-4b32-b438-92ffe02495d1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ----------------------------------\n",
    "# 5. Dimensjonstabell: Tid\n",
    "# ----------------------------------\n",
    "\n",
    "\n",
    "\n",
    "from pyspark.sql.functions import to_date, hour, minute\n",
    "from pyspark.sql import Row\n",
    "\n",
    "# Create an empty DataFrame with the same schema\n",
    "#empty_df = spark.createDataFrame([], spark.table(\"default.dim_tid\").schema)\n",
    "\n",
    "# Overwrite the table with empty data\n",
    "#empty_df.write.mode(\"overwrite\").saveAsTable(\"default.dim_tid\")\n",
    "\n",
    "\n",
    "# Bygg batch kun basert på nyeste data\n",
    "dim_tid_batch = df_cleaned.select(\n",
    "    to_date(\"timestamp\").cast(\"date\").alias(\"dato\"),\n",
    "    hour(\"timestamp\").cast(\"int\").alias(\"time\"),\n",
    "    minute(\"timestamp\").cast(\"int\").alias(\"minutt\")\n",
    ").distinct()\n",
    "\n",
    "# Sjekk om tabellen finnes\n",
    "if \"dim_tid\" in [t.name for t in spark.catalog.listTables(\"default\")]:\n",
    "    # Hent eksisterende tabell\n",
    "    existing_dim_tid = spark.table(\"default.dim_tid\").select(\"dato\", \"time\", \"minutt\")\n",
    "\n",
    "    # Finn tidspunkt som ikke finnes fra før\n",
    "    delta_tid = dim_tid_batch.join(existing_dim_tid, [\"dato\", \"time\", \"minutt\"], how=\"left_anti\")\n",
    "\n",
    "    # Legg til nye rader hvis noen er nye\n",
    "    if delta_tid.count() > 0:\n",
    "        delta_tid.write.format(\"delta\").mode(\"append\").saveAsTable(\"default.dim_tid\")\n",
    "else:\n",
    "    # Hvis tabellen ikke finnes, opprett den\n",
    "    dim_tid_batch.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"default.dim_tid\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "10965ebd-3416-4c55-bc14-9d5f59c37177",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ----------------------------------\n",
    "# 6. Faktatabell: Parkeringskapasitet\n",
    "# ----------------------------------\n",
    "from pyspark.sql.functions import to_date, hour, minute\n",
    "\n",
    "# Hent 9 observasjoner per kjøring (én per lokasjon)\n",
    "new_fakt = df_cleaned.select(\n",
    "    to_date(\"timestamp\").alias(\"dato\"),\n",
    "    date_format(\"timestamp\", \"HH:mm\").alias(\"klokkeslett\"),  # ← Ny kolonne\n",
    "    \"Sted\",\n",
    "    \"Antall_ledige_plasser\",\n",
    "    \"timestamp\"\n",
    ")\n",
    "\n",
    "# Append for å lagre hver kjørings observasjoner\n",
    "new_fakt.write.format(\"delta\").mode(\"append\").saveAsTable(\"default.fakt_parkering\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "705591b5-2436-4cbb-92c0-4a640233ec90",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ----------------------------------\n",
    "# 7. Visualisering: Utvikling per lokasjon (én dag)\n",
    "# ----------------------------------\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import matplotlib.dates as mdates\n",
    "\n",
    "# --- Hent full historikk fra faktatabellen ---\n",
    "df_plot = spark.table(\"default.fakt_parkering\")\n",
    "\n",
    "# Konverter til Pandas\n",
    "df_pd = df_plot.toPandas()\n",
    "\n",
    "# --- Konverter timestamp til datetime ---\n",
    "df_pd['datetime'] = pd.to_datetime(df_pd['timestamp'])\n",
    "\n",
    "# --- Velg én dag for visualisering ---\n",
    "valgt_dato = \"2025-06-11\"\n",
    "df_pd['dato'] = df_pd['datetime'].dt.date\n",
    "df_plot_dag = df_pd[df_pd['dato'] == pd.to_datetime(valgt_dato).date()]\n",
    "\n",
    "# Sjekk om vi har noen data\n",
    "unike_steder = df_plot_dag['Sted'].unique()\n",
    "\n",
    "if len(unike_steder) > 0:\n",
    "    fig, axs = plt.subplots(len(unike_steder), 1, figsize=(12, 4 * len(unike_steder)), sharex=True)\n",
    "\n",
    "    if len(unike_steder) == 1:\n",
    "        axs = [axs]\n",
    "\n",
    "    for ax, sted in zip(axs, unike_steder):\n",
    "        subset = df_plot_dag[df_plot_dag['Sted'] == sted].sort_values('datetime')\n",
    "        ax.plot(subset['datetime'], subset['Antall_ledige_plasser'], marker='o', linestyle='-')\n",
    "        ax.set_title(f\"Ledige plasser - {sted}\")\n",
    "        ax.set_ylabel(\"Antall ledige\")\n",
    "        ax.grid(True)\n",
    "\n",
    "\n",
    "    axs[-1].xaxis.set_major_locator(mdates.AutoDateLocator())\n",
    "    axs[-1].xaxis.set_major_formatter(mdates.DateFormatter(\"%H:%M\"))\n",
    "\n",
    "    plt.xlabel(\"Tid (klokkeslett)\")\n",
    "    fig.autofmt_xdate(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Ingen data funnet for valgt dato:\", valgt_dato)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8827ffb7-935c-4692-b710-29a0ab36b204",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "use catalog `hive_metastore`; select * from `default`.`dim_tid` limit 100;"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 7254531232939827,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Stavanger_parkering",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
