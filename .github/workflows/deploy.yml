# Workflow: Deploy to Databricks Production
# Triggers when code is pushed to the 'main' branch
#v

name: Deploy to Databricks Prod

on:
  push:
    branches:
      - main  # Only deploy when the main branch is updated

jobs:
  deploy:
    runs-on: ubuntu-latest  # Run workflow on latest Ubuntu VM

    permissions:
      contents: write  # Required to push git tags

    env:
      # Environment variables for Databricks credentials (stored as GitHub secrets)
      DATABRICKS_HOST: ${{ secrets.DATABRICKS_HOST }}
      DATABRICKS_TOKEN: ${{ secrets.DATABRICKS_TOKEN }}

    steps:
      # Step 1: Checkout the repository code
      - uses: actions/checkout@v3

      # Step 2: Install necessary Python packages + CLI
      - name: Install dependencies
        run: |
          sudo apt-get update
          sudo apt-get install -y jq
          python3 -m pip install --upgrade databricks-cli

      # Step 3: Configure Databricks CLI
      - name: Configure Databricks CLI
        run: |
          mkdir -p ~/.databricks
          cat > ~/.databrickscfg << 'EOF'
          [DEFAULT]
          host = ${{ secrets.DATABRICKS_HOST }}
          token = ${{ secrets.DATABRICKS_TOKEN }}
          jobs-api-version = 2.1
          EOF
          # Ensure CLI uses the new Jobs API
          databricks jobs configure --version=2.1


      # Step 4: Deploy Notebooks (CLI)
      - name: Deploy notebooks
        run: |
          for nb in notebooks/*.ipynb; do
            target="/Shared/$(basename "$nb" .ipynb)_prod"
            echo "Deploying $nb -> $target"
            databricks workspace import "$nb" "$target" -f SOURCE -l PYTHON --overwrite
          done

      # Step 5: Deploy jobs (create if missing, update if existing)
      - name: Deploy jobs
        run: |
          # Loop through all JSON files in the 'jobs' directory.
          # Each JSON file is expected to define a single Databricks job.
          for job_file in jobs/*.json; do
            
            # Use 'jq' to extract the job's name from the current JSON file.
            # The '-r' flag ensures the output is a raw string without extra quotes.
            job_name=$(jq -r '.name' "$job_file")

            # The core logic: check if a job with the same name already exists in Databricks.
            # 1. 'databricks jobs list --output JSON': Retrieves a list of all jobs in the workspace as a JSON object.
            # 2. '|': Pipes the JSON output to the next command, 'jq'.
            # 3. 'jq -r ".jobs[] | select(.settings.name==\"$job_name\") | .job_id"':
            #    - ".jobs[]": Navigates into the 'jobs' array to process each job object.
            #    - "select(.settings.name==\"$job_name\")": Filters the list to find jobs where the 'name' field
            #      inside the 'settings' object matches the job name from our file.
            #    - ".job_id": Extracts the unique ID of the matching job.
            # 4. '| head -n 1': A safety measure to take only the first matching job ID. This is crucial
            #    to handle potential duplicates and ensures a single ID is passed to the next command.
            existing_job_id=$(databricks jobs list --output JSON | jq -r ".jobs[] | select(.settings.name==\"$job_name\") | .job_id" | head -n 1)

            # Conditional check: if a job ID was found, update the existing job; otherwise, create a new one.
            # The '-n' flag checks if the string is non-empty. We also check for the literal string "null".
            if [ -n "$existing_job_id" ] && [ "$existing_job_id" != "null" ]; then
              echo "Updating existing job: $job_name (id=$existing_job_id)"
              # 'databricks jobs reset' updates an existing job using its ID and a new JSON file.
              databricks jobs reset --job-id "$existing_job_id" --json-file "$job_file"
            else
              echo "Creating new job: $job_name"
              # 'databricks jobs create' creates a new job from a JSON file.
              databricks jobs create --json-file "$job_file"
            fi
          done


      # Step 6: Deploy Dashboards (Python script, REST API)
      - name: Deploy Lakeview dashboards
        run: python3 scripts/deploy_to_databricks.py

      # Step 7: Tag the deployment in GitHub for traceability
      - name: Create Git tag for release
        run: |
          git config --global user.email "github-actions[bot]@users.noreply.github.com"
          git config --global user.name "github-actions[bot]"
          git tag "prod-deploy-$(date +'%Y-%m-%d-%H-%M')" -m "Deployed to production"
          git push origin --tags

      # Step 8: Notify success
      - name: Notify on success
        if: success()
        run: echo "✅ Deployment to production successful!"

      # Step 9: Notify failure
      - name: Notify on failure
        if: failure()
        run: echo "❌ Deployment to production failed!"