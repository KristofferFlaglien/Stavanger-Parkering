# Workflow: Deploy to Databricks Production
# Triggers when code is pushed to the 'main' branch
#v


name: Deploy to Databricks Prod

on:
  push:
    branches:
      - main  # Only deploy when the main branch is updated

jobs:
  deploy:
    runs-on: ubuntu-latest  # Run workflow on latest Ubuntu VM

    permissions:
      contents: write  # Required to push git tags

    env:
      # Environment variables for Databricks credentials (stored as GitHub secrets)
      DATABRICKS_HOST: ${{ secrets.DATABRICKS_HOST }}
      DATABRICKS_TOKEN: ${{ secrets.DATABRICKS_TOKEN }}

    steps:
      # Step 1: Checkout the repository code
      - uses: actions/checkout@v3

      # Step 2: Install necessary Python packages + CLI
      - name: Install dependencies
        run: |
          sudo apt-get update
          sudo apt-get install -y jq
          python3 -m pip install --upgrade databricks-cli

      # Step 3: Configure Databricks CLI
      - name: Configure Databricks CLI
        run: |
          mkdir -p ~/.databricks
          cat > ~/.databrickscfg << 'EOF'
          [DEFAULT]
          host = ${{ secrets.DATABRICKS_HOST }}
          token = ${{ secrets.DATABRICKS_TOKEN }}
          jobs-api-version = 2.1
          EOF
          # Ensure CLI uses the new Jobs API
          databricks jobs configure --version=2.1


      # Step 4: Deploy Notebooks (CLI)
      - name: Deploy notebooks
        run: |
          for nb in notebooks/*.ipynb; do
            target="/Shared/$(basename "$nb" .ipynb)_prod"
            echo "Deploying $nb -> $target"
            databricks workspace import "$nb" "$target" -f SOURCE -l PYTHON --overwrite
          done

      # Step 5: Deploy jobs (create if missing, update if existing)
      - name: Deploy jobs
        run: |
          for job_file in jobs/*.json; do
            job_name=$(jq -r '.name' "$job_file")

            # Query for a single job by name. This is more reliable as it doesn't depend on a full job list.
            # The '--jobs-api-version' flag is for the top-level configuration, not a sub-command.
            existing_job_id=$(databricks jobs list --output JSON | jq -r ".jobs[] | select(.settings.name==\"$job_name\") | .job_id" | head -n 1)

            if [ -n "$existing_job_id" ] && [ "$existing_job_id" != "null" ]; then
              echo "Updating existing job: $job_name (id=$existing_job_id)"
              databricks jobs reset --job-id "$existing_job_id" --json-file "$job_file"
            else
              echo "Creating new job: $job_name"
              databricks jobs create --json-file "$job_file"
            fi
          done


      # Step 6: Deploy Dashboards (Python script, REST API)
      - name: Deploy Lakeview dashboards
        run: python3 scripts/deploy_to_databricks.py

      # Step 7: Tag the deployment in GitHub for traceability
      - name: Create Git tag for release
        run: |
          git config --global user.email "github-actions[bot]@users.noreply.github.com"
          git config --global user.name "github-actions[bot]"
          git tag "prod-deploy-$(date +'%Y-%m-%d-%H-%M')" -m "Deployed to production"
          git push origin --tags

      # Step 8: Notify success
      - name: Notify on success
        if: success()
        run: echo "✅ Deployment to production successful!"

      # Step 9: Notify failure
      - name: Notify on failure
        if: failure()
        run: echo "❌ Deployment to production failed!"