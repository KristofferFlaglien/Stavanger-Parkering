# This workflow triggers when code is pushed to the main branch
name: Deploy to Databricks Prod

on:
  push:
    branches:
      - main  # Only run when pushing to main branch (production deployments)

jobs:
  deploy:
    runs-on: ubuntu-latest  # Use GitHub's Ubuntu virtual machine
    permissions:
      contents: write  # Allow this workflow to write to the repository (needed for git tags)
    env:
      # Load secrets from GitHub repository settings
      DATABRICKS_HOST: ${{ secrets.DATABRICKS_HOST }}      # Your Databricks workspace URL
      DATABRICKS_TOKEN: ${{ secrets.DATABRICKS_TOKEN }}    # Your personal access token

    steps:
      # STEP 1: Get the code from your repository
      - uses: actions/checkout@v3
        # This downloads your repository's code to the GitHub runner

      # STEP 2: Install the Databricks command-line tool
      - name: Install Databricks CLI
        run: pip install --upgrade databricks-cli
        # This installs the databricks CLI so we can interact with Databricks

      # STEP 3: Configure the Databricks CLI with your credentials
      - name: Configure Databricks CLI
        run: |
          # Create a configuration file that tells the CLI which workspace to use
          cat > ~/.databrickscfg << EOF
          [DEFAULT]
          host = ${{ secrets.DATABRICKS_HOST }}    # Where to connect
          token = ${{ secrets.DATABRICKS_TOKEN }}  # How to authenticate
          EOF
        # Without this step, the CLI wouldn't know which Databricks workspace to use

      # STEP 4: Deploy your Jupyter notebooks to Databricks
      - name: Deploy notebooks to Databricks
        run: |
          set -euo pipefail  # Exit immediately if any command fails
          # Loop through all .ipynb files in the notebooks folder
          for file in ./notebooks/*.ipynb; do
            echo "Deploying $file"  # Show which file we're deploying
            filename=$(basename "$file" .ipynb)  # Get filename without .ipynb extension
            # Upload the notebook to Databricks workspace
            # "/Shared/${filename}_prod" = destination path in Databricks
            # -f SOURCE = format as source code
            # -l PYTHON = language is Python  
            # --overwrite = replace if it already exists
            databricks workspace import "$file" "/Shared/${filename}_prod" -f SOURCE -l PYTHON --overwrite
          done

      # STEP 5: Deploy dashboards (NEW and CORRECT METHOD)
      - name: Deploy Lakeview dashboards
        run: |
          set -euo pipefail
          # Loop through all dashboard files and deploy them using the Databricks CLI
          for file in ./dashboards/*.lvdash.json; do
            echo "Deploying dashboard $file"
            databricks workspace import "$file" "/Shared/$(basename "$file")" --overwrite
          done

      # STEP 6: Create a git tag to mark this deployment
      - name: Create Git tag for release
        run: |
          # Configure git with a user (required to make commits/tags)
          # These are the "official" GitHub Actions bot credentials
          git config --global user.email "github-actions[bot]@users.noreply.github.com"
          git config --global user.name "github-actions[bot]"
          
          # Create a tag with current timestamp to mark this deployment
          git tag "prod-deploy-$(date +'%Y-%m-%d-%H-%M')" -m "Deployed to production"
          
          # Push the tag back to GitHub so you can see deployment history
          git push origin --tags

      # STEP 7: Success notification
      - name: Notify on success
        if: success()  # Only run if all previous steps succeeded
        run: echo "✅ Deployment to production successful!"

      # STEP 8: Failure notification  
      - name: Notify on failure
        if: failure()  # Only run if any previous step failed
        run: echo "❌ Deployment to production failed!"
