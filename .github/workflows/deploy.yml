# Workflow: Deploy to Databricks Production
# Triggers when code is pushed to the 'main' branch

name: Deploy to Databricks Prod

on:
  push:
    branches:
      - main  # Only deploy when the main branch is updated

jobs:
  deploy:
    runs-on: ubuntu-latest  # Run workflow on latest Ubuntu VM

    permissions:
      contents: write  # Required to push git tags

    env:
      # Environment variables for Databricks credentials (stored as GitHub secrets)
      DATABRICKS_HOST: ${{ secrets.DATABRICKS_HOST }}
      DATABRICKS_TOKEN: ${{ secrets.DATABRICKS_TOKEN }}

    steps:
      # Step 1: Checkout the repository code
      - uses: actions/checkout@v3

      # Step 2: Install necessary Python packages + CLI
      - name: Install dependencies
        run: |
          sudo apt-get update
          sudo apt-get install -y jq
          python3 -m pip install --upgrade databricks-sdk

      # Step 3: Configure Databricks CLI
      - name: Configure Databricks CLI
        run: |
          mkdir -p ~/.databricks
          cat > ~/.databrickscfg << EOF
          [DEFAULT]
          host = ${{ secrets.DATABRICKS_HOST }}
          token = ${{ secrets.DATABRICKS_TOKEN }}
          jobs-api-version = 2.1
          EOF


      # Step 4: Deploy Notebooks (CLI)
      - name: Deploy notebooks
        run: |
          for nb in notebooks/*.ipynb; do
            target="/Shared/$(basename "$nb" .ipynb)_prod"
            echo "Deploying $nb -> $target"
            databricks workspace import "$nb" "$target" -f SOURCE -l PYTHON --overwrite
          done



      # Step 5: Deploy jobs (create if missing, update if existing)
      - name: Deploy jobs
        run: |
          # Loop through all job configuration files
          for job_file in jobs/*.json; do
            job_name=$(jq -r '.name' "$job_file")
            echo "Processing job: $job_name"

            # Try to update the job first.
            # The '||' operator executes the next command only if the first one fails.

            databricks jobs update --json-file "$job_file" || {
              # If update fails, it means the job doesn't exist. So, create it.

              echo "Job '$job_name' not found. Creating a new job."
              databricks jobs create --json-file "$job_file"
            }
          done

      # Step 6: Deploy Dashboards (Python script, REST API)
      - name: Deploy Lakeview dashboards
        run: python3 scripts/deploy_to_databricks.py

      # Step 7: Tag the deployment in GitHub for traceability
      - name: Create Git tag for release
        run: |
          git config --global user.email "github-actions[bot]@users.noreply.github.com"
          git config --global user.name "github-actions[bot]"
          git tag "prod-deploy-$(date +'%Y-%m-%d-%H-%M')" -m "Deployed to production"
          git push origin --tags

      # Step 8: Notify success
      - name: Notify on success
        if: success()
        run: echo "✅ Deployment to production successful!"

      # Step 9: Notify failure
      - name: Notify on failure
        if: failure()
        run: echo "❌ Deployment to production failed!"


